{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install stable-baselines[mpi]==2.10.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHlPnhOseutA",
        "outputId": "ba86de92-3dee-4522-848d-1f19409fd43e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow 1.x selected.\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting stable-baselines[mpi]==2.10.0\n",
            "  Downloading stable_baselines-2.10.0-py3-none-any.whl (248 kB)\n",
            "\u001b[K     |████████████████████████████████| 248 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.1.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (4.1.2.30)\n",
            "Requirement already satisfied: cloudpickle>=0.5.5 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.3.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.4.1)\n",
            "Requirement already satisfied: gym[atari,classic_control]>=0.11 in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (0.17.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from stable-baselines[mpi]==2.10.0) (1.21.6)\n",
            "Requirement already satisfied: mpi4py in /tensorflow-1.15.2/python3.7 (from stable-baselines[mpi]==2.10.0) (3.0.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (7.1.2)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.2.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari,classic_control]>=0.11->stable-baselines[mpi]==2.10.0) (0.16.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (1.4.3)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->stable-baselines[mpi]==2.10.0) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->stable-baselines[mpi]==2.10.0) (4.1.1)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->stable-baselines[mpi]==2.10.0) (2022.1)\n",
            "Installing collected packages: stable-baselines\n",
            "  Attempting uninstall: stable-baselines\n",
            "    Found existing installation: stable-baselines 2.2.1\n",
            "    Uninstalling stable-baselines-2.2.1:\n",
            "      Successfully uninstalled stable-baselines-2.2.1\n",
            "Successfully installed stable-baselines-2.10.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Constants"
      ],
      "metadata": {
        "id": "9bhKTwNw12Nt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE_SPEED = 100 # Km/h\n",
        "PRECISION = 1 #Km"
      ],
      "metadata": {
        "id": "DS6rp6eP17OL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Utils - Location"
      ],
      "metadata": {
        "id": "RZutjlwP2Ege"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from geopy.distance import geodesic\n",
        "\n",
        "# comment the line below to generate different random numbers\n",
        "random.seed(1)\n",
        "\n",
        "\n",
        "def generate_random_coordinates():\n",
        "    point = {\n",
        "        'x': random.uniform(-180.0, 180.0),\n",
        "        'y': random.uniform(-90.0, 90.0)\n",
        "    }\n",
        "    return point\n",
        "\n",
        "\n",
        "def calculate_distance(point1, point2):\n",
        "    p1_as_arr = np.array((point1['x'], point1['y']))\n",
        "    p2_as_arr = np.array((point2['x'], point2['y']))\n",
        "    return np.linalg.norm(p1_as_arr - p2_as_arr)\n",
        "\n",
        "\n",
        "def calculate_geodesic_distance(point1, point2):\n",
        "    return geodesic((point1['y'],point1['x']), (point2['y'],point2['x'])).kilometers\n",
        "\n",
        "\n",
        "def calculate_geodesic_movement(point, direction, distance):\n",
        "    new_point = geodesic(kilometers=distance).destination((point['y'], point['x']), bearing=direction)\n",
        "    return { 'x': new_point[1], 'y': new_point[0] }\n",
        "\n",
        "\n",
        "def check_limits(pos_x, pos_y, max_value, min_value=0.0):\n",
        "    return min_value <= pos_x <= max_value and min_value <= pos_y <= max_value\n",
        "\n",
        "\n",
        "def update_position(position_object, new_position):\n",
        "    if not isinstance(position_object, dict):\n",
        "        raise TypeError\n",
        "\n",
        "    for coordinate in position_object.keys():\n",
        "        position_object[coordinate] = new_position[coordinate]\n",
        "\n",
        "    return position_object\n"
      ],
      "metadata": {
        "id": "7PUVFsx72KBm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Controllers - UAV"
      ],
      "metadata": {
        "id": "s09IguFm2Myw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class UAVController:\n",
        "    def __init__(self, uav_id, position):\n",
        "        if not isinstance(position, dict):\n",
        "            raise TypeError(\n",
        "                \"position must be a dict { 'x': float, 'y': float }\")\n",
        "\n",
        "        self._id = uav_id\n",
        "        self._position = position\n",
        "\n",
        "    def get_id(self):\n",
        "        return self._id\n",
        "\n",
        "    def get_position(self):\n",
        "        return self._position\n",
        "    \n",
        "    def set_position(self, position):\n",
        "        self._position = position"
      ],
      "metadata": {
        "id": "aqoZTDYu2bmc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Controllers - Host"
      ],
      "metadata": {
        "id": "ZnXgjyzG2hrg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HostController:\n",
        "    def __init__(self, host_id, position):\n",
        "        if not isinstance(position, dict):\n",
        "            raise TypeError(\n",
        "                \"position must be a dict { 'x': float, 'y': float }\")\n",
        "\n",
        "        self._id = host_id\n",
        "        self._position = position\n",
        "\n",
        "    def get_id(self):\n",
        "        return self._id\n",
        "\n",
        "    def get_position(self):\n",
        "        return self._position\n",
        "\n",
        "    def set_position(self, position):\n",
        "        self._position = position"
      ],
      "metadata": {
        "id": "wsGU2wnE2pzc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Controllers - Simulation"
      ],
      "metadata": {
        "id": "y-fmXK0K23Az"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimulationController:\n",
        "    def __init__(self, host_quantity=2, uav_quantity=1, **kw_args):\n",
        "        self._hosts = []\n",
        "        self._uavs = []\n",
        "        self._center_of_mass = None\n",
        "\n",
        "        if 'hosts' in kw_args:\n",
        "            self._hosts = kw_args['hosts']\n",
        "        else:\n",
        "            for host_index in range(host_quantity):\n",
        "                position = generate_random_coordinates()\n",
        "                id = kw_args['hosts_names'][host_index] \\\n",
        "                    if 'hosts_names' in kw_args \\\n",
        "                    else \"host_{}\".format(host_index)\n",
        "                new_host = HostController(id, position)\n",
        "                self._hosts.append(new_host)\n",
        "\n",
        "        if 'uavs' in kw_args:\n",
        "            self._uavs = kw_args['uavs']\n",
        "        else:\n",
        "            for uav_index in range(uav_quantity):\n",
        "                position = generate_random_coordinates()\n",
        "                id = \"uav_{}\".format(uav_index)\n",
        "                new_uav = UAVController(id, position)\n",
        "                self._uavs.append(new_uav)\n",
        "\n",
        "        self.calculate_center_of_mass()\n",
        "\n",
        "    def get_hosts(self):\n",
        "        return self._hosts\n",
        "\n",
        "    def get_uavs(self):\n",
        "        return self._uavs\n",
        "\n",
        "    def get_center_of_mass(self):\n",
        "        self.calculate_center_of_mass()\n",
        "        return self._center_of_mass\n",
        "\n",
        "    def calculate_center_of_mass(self):\n",
        "        x_sum = 0.0\n",
        "        y_sum = 0.0\n",
        "        hosts_quantity = len(self._hosts)\n",
        "\n",
        "        for host in self._hosts:\n",
        "            position = host.get_position()\n",
        "            x_sum += position['x']\n",
        "            y_sum += position['y']\n",
        "\n",
        "        self._center_of_mass = {\n",
        "            'x': x_sum / hosts_quantity,\n",
        "            'y': y_sum / hosts_quantity\n",
        "        }"
      ],
      "metadata": {
        "id": "wf8d05Jy27eE"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment"
      ],
      "metadata": {
        "id": "c26veYmUeQFc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from gym import Env\n",
        "from gym.spaces import Discrete, Box\n",
        "\n",
        "# Possible actions:\n",
        "#     0: Stay stopped\n",
        "#     1: Move up\n",
        "#     2: Move right\n",
        "#     3: Move down\n",
        "#     4: Move left\n",
        "ACTIONS = {\n",
        "    0: 'stay_stopped',\n",
        "    1: 'move_up',\n",
        "    2: 'move_right',\n",
        "    3: 'move_down',\n",
        "    4: 'move_left',\n",
        "}\n",
        "\n",
        "class UAVPlacerEnv(Env):\n",
        "    def __init__(self):\n",
        "        self.action_space = Discrete(5)\n",
        "        boundaries = np.array(\n",
        "            [\n",
        "                180.0,\n",
        "                90.0,\n",
        "                180.0,\n",
        "                90.0\n",
        "            ],\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "        self.observation_space = Box(low=-boundaries, high=boundaries, dtype=np.float32)\n",
        "        self.speed = BASE_SPEED\n",
        "\n",
        "        self._start_simulation()\n",
        "        uav_position = self.simulation.get_uavs()[0].get_position()\n",
        "        center_of_mass = self.simulation.get_center_of_mass()\n",
        "\n",
        "        self.state = np.array(\n",
        "            [\n",
        "                uav_position['x'],\n",
        "                uav_position['y'],\n",
        "                center_of_mass['x'],\n",
        "                center_of_mass['y'],\n",
        "            ],\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "\n",
        "    def _start_simulation(self) -> None:\n",
        "        random_coordinates = generate_random_coordinates()\n",
        "        host1 = HostController(\"host_1\", random_coordinates)\n",
        "        host2 = HostController(\"host_2\", {'x': random_coordinates['x']+0.08, 'y': random_coordinates['y']+0.08})\n",
        "\n",
        "        uav_position = {\n",
        "            'x': random.uniform(host1.get_position()['x'],host2.get_position()['x']),\n",
        "            'y': random.uniform(host1.get_position()['y'],host2.get_position()['y'])\n",
        "        }\n",
        "\n",
        "        uav = UAVController(\"uav_1\", uav_position)\n",
        "        self.simulation = SimulationController(hosts=[host1,host2], uavs=[uav])\n",
        "\n",
        "\n",
        "    def _calculate_uav_distance_to_center_of_mass(self) -> float:\n",
        "        uav = self.simulation.get_uavs()[0]\n",
        "        center_of_mass = self.simulation.get_center_of_mass()\n",
        "\n",
        "        return calculate_geodesic_distance(uav.get_position(), center_of_mass)\n",
        "\n",
        "\n",
        "    def _calculate_reward(self, current_distance, previous_distance):\n",
        "        if current_distance < previous_distance:\n",
        "            return 1\n",
        "        elif current_distance >= previous_distance:\n",
        "            return -1\n",
        "\n",
        "\n",
        "    def _move_uav(self, action):\n",
        "        uav = self.simulation.get_uavs()[0]\n",
        "        current_position = uav.get_position()\n",
        "        if ACTIONS.get(action) == 'move_up':\n",
        "            direction = 0\n",
        "        elif ACTIONS.get(action) == 'move_right':\n",
        "            direction = 90\n",
        "        elif ACTIONS.get(action) == 'move_down':\n",
        "            direction = 180\n",
        "        elif ACTIONS.get(action) == 'move_left':\n",
        "            direction = -90\n",
        "\n",
        "        if ACTIONS.get(action) != 'stay_stopped':\n",
        "            new_position = calculate_geodesic_movement(current_position, direction, BASE_SPEED/60.0)\n",
        "            uav.set_position(new_position)\n",
        "\n",
        "    def step(self, action):\n",
        "        previous_distance = self._calculate_uav_distance_to_center_of_mass()\n",
        "\n",
        "        # Move UAV\n",
        "        self._move_uav(action)\n",
        "\n",
        "        # Calculate distance to center of mass\n",
        "        current_distance = self._calculate_uav_distance_to_center_of_mass()\n",
        "\n",
        "        # Calculate reward\n",
        "        reward = self._calculate_reward(current_distance, previous_distance)\n",
        "\n",
        "        # Check if is done\n",
        "        if current_distance <= PRECISION:\n",
        "            done = True\n",
        "        else:\n",
        "            done = False\n",
        "\n",
        "        uav = self.simulation.get_uavs()[0]\n",
        "        uav_position = uav.get_position()\n",
        "        center_of_mass = self.simulation.get_center_of_mass()\n",
        "        self.state = np.array(\n",
        "            [\n",
        "                uav_position['x'],\n",
        "                uav_position['y'],\n",
        "                center_of_mass['x'],\n",
        "                center_of_mass['y'],\n",
        "            ],\n",
        "            dtype=np.float32\n",
        "        )\n",
        "        info = {\n",
        "            'current_distance': current_distance,\n",
        "        }\n",
        "\n",
        "        return self.state, reward, done, info\n",
        "\n",
        "\n",
        "    def render(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        self._start_simulation()\n",
        "        uav_position = self.simulation.get_uavs()[0].get_position()\n",
        "        center_of_mass = self.simulation.get_center_of_mass()\n",
        "\n",
        "        self.state = np.array(\n",
        "            [\n",
        "                uav_position['x'],\n",
        "                uav_position['y'],\n",
        "                center_of_mass['x'],\n",
        "                center_of_mass['y'],\n",
        "            ],\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "        return self.state"
      ],
      "metadata": {
        "id": "6FR-59dXeTTj"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DOpP_B0-LXm"
      },
      "source": [
        "from stable_baselines.common.env_checker import check_env\n",
        "env = UAVPlacerEnv()\n",
        "# If the environment don't follow the interface, an error will be thrown\n",
        "check_env(env, warn=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test environment"
      ],
      "metadata": {
        "id": "vtz9Z2eKhQ3T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "obs = env.reset()\n",
        "\n",
        "print(env.observation_space)\n",
        "print(env.action_space)\n",
        "\n",
        "n_steps = 20\n",
        "for step in range(n_steps):\n",
        "  print(\"Step {}\".format(step + 1))\n",
        "  action = env.action_space.sample()\n",
        "  print(\"Action {}\".format(ACTIONS.get(action)))\n",
        "\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  print('obs=', obs, 'reward=', reward, 'done=', done, 'info=', info)\n",
        "  if done:\n",
        "    print(\"Goal reached!\", \"reward=\", reward)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cAjSzFNEguUc",
        "outputId": "4c3c931c-c3b8-4e22-9e9d-c7585401e3cb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Box(-180.0, 180.0, (4,), float32)\n",
            "Discrete(5)\n",
            "Step 1\n",
            "Action move_right\n",
            "obs= [ 96.41297  -89.562935  94.46083  -89.58091 ] reward= -1 done= False info= {'current_distance': 2.5854604726508623}\n",
            "Step 2\n",
            "Action move_up\n",
            "obs= [ 96.41297 -89.54801  94.46083 -89.58091] reward= -1 done= False info= {'current_distance': 4.030679481491554}\n",
            "Step 3\n",
            "Action move_up\n",
            "obs= [ 96.41297 -89.53309  94.46083 -89.58091] reward= -1 done= False info= {'current_distance': 5.600330682198745}\n",
            "Step 4\n",
            "Action move_left\n",
            "obs= [ 94.58249 -89.53285  94.46083 -89.58091] reward= 1 done= False info= {'current_distance': 5.369015674199385}\n",
            "Step 5\n",
            "Action move_up\n",
            "obs= [ 94.58249 -89.51793  94.46083 -89.58091] reward= -1 done= False info= {'current_distance': 7.035464376945541}\n",
            "Step 6\n",
            "Action move_right\n",
            "obs= [ 96.355446 -89.5177    94.46083  -89.58091 ] reward= -1 done= False info= {'current_distance': 7.253057972372776}\n",
            "Step 7\n",
            "Action move_right\n",
            "obs= [ 98.12755 -89.51746  94.46083 -89.58091] reward= -1 done= False info= {'current_distance': 7.780951664364594}\n",
            "Step 8\n",
            "Action move_up\n",
            "obs= [ 98.12755 -89.50255  94.46083 -89.58091] reward= -1 done= False info= {'current_distance': 9.341365773116342}\n",
            "Step 9\n",
            "Action stay_stopped\n",
            "obs= [ 98.12755 -89.50255  94.46083 -89.58091] reward= -1 done= False info= {'current_distance': 9.341365773116342}\n",
            "Step 10\n",
            "Action move_right\n",
            "obs= [ 99.84571 -89.50232  94.46083 -89.58091] reward= -1 done= False info= {'current_distance': 10.000874860525078}\n",
            "Step 11\n",
            "Action move_down\n",
            "obs= [ 99.84571 -89.51724  94.46083 -89.58091] reward= 1 done= False info= {'current_distance': 8.535051644573093}\n",
            "Step 12\n",
            "Action stay_stopped\n",
            "obs= [ 99.84571 -89.51724  94.46083 -89.58091] reward= -1 done= False info= {'current_distance': 8.535051644573093}\n",
            "Step 13\n",
            "Action move_down\n",
            "obs= [ 99.84571  -89.532166  94.46083  -89.58091 ] reward= 1 done= False info= {'current_distance': 7.157672097698275}\n",
            "Step 14\n",
            "Action move_right\n",
            "obs= [101.67258 -89.53193  94.46083 -89.58091] reward= -1 done= False info= {'current_distance': 8.285703506663081}\n",
            "Step 15\n",
            "Action move_right\n",
            "obs= [103.49851 -89.53169  94.46083 -89.58091] reward= -1 done= False info= {'current_distance': 9.540350564957942}\n",
            "Step 16\n",
            "Action move_right\n",
            "obs= [105.323524 -89.53145   94.46083  -89.58091 ] reward= -1 done= False info= {'current_distance': 10.876859958794716}\n",
            "Step 17\n",
            "Action move_left\n",
            "obs= [103.499435 -89.53121   94.46083  -89.58091 ] reward= 1 done= False info= {'current_distance': 9.574905852294837}\n",
            "Step 18\n",
            "Action move_right\n",
            "obs= [105.3226   -89.530975  94.46083  -89.58091 ] reward= -1 done= False info= {'current_distance': 10.907295890877641}\n",
            "Step 19\n",
            "Action move_up\n",
            "obs= [105.3226  -89.51605  94.46083 -89.58091] reward= -1 done= False info= {'current_distance': 11.963822247954049}\n",
            "Step 20\n",
            "Action move_down\n",
            "obs= [105.3226   -89.530975  94.46083  -89.58091 ] reward= 1 done= False info= {'current_distance': 10.907295890877641}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trying it with stable-baselines"
      ],
      "metadata": {
        "id": "aJMi6L4Uhgbm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines import DQN\n",
        "from stable_baselines.common.evaluation import evaluate_policy\n",
        "from stable_baselines.common.cmd_util import make_vec_env\n",
        "\n",
        "# Instantiate the env\n",
        "env = UAVPlacerEnv()\n",
        "# wrap it\n",
        "env = make_vec_env(lambda: env, n_envs=1)"
      ],
      "metadata": {
        "id": "JNGM0NFahe60"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the agent\n",
        "dqn_model = DQN(\"MlpPolicy\", \n",
        "                env, \n",
        "                learning_rate=4e-3, \n",
        "                buffer_size=10000, \n",
        "                exploration_fraction=0.2, \n",
        "                exploration_final_eps=0.07, \n",
        "                train_freq=16, \n",
        "                batch_size=128, \n",
        "                double_q=True, \n",
        "                prioritized_replay=True, \n",
        "                prioritized_replay_alpha=0.6, \n",
        "                prioritized_replay_beta0=0.4, \n",
        "                prioritized_replay_beta_iters=None, \n",
        "                prioritized_replay_eps=1e-06, \n",
        "                verbose=1, \n",
        "                tensorboard_log=None, \n",
        "                seed=1).learn(int(1e6), log_interval=100)"
      ],
      "metadata": {
        "id": "mT4mj-I6hofL"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the trained agent\n",
        "obs = env.reset()\n",
        "n_steps = 1000\n",
        "total_reward = 0\n",
        "for step in range(n_steps):\n",
        "  action, _ = dqn_model.predict(obs, deterministic=True)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  total_reward += reward\n",
        "  if step % 100 == 0:\n",
        "    print(\"Step {}\".format(step + 1))\n",
        "    print(\"Action: \", action)\n",
        "    print('obs=', obs, 'reward=', reward, 'done=', done, 'info=', info)\n",
        "  if done:\n",
        "    # Note that the VecEnv resets automatically\n",
        "    # when a done signal is encountered\n",
        "    print(\"Goal reached!\", \"reward=\", total_reward)\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIpnHVgch9BP",
        "outputId": "485aa4cd-dc3f-4675-cae2-5da88adee1ba"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step 1\n",
            "Action:  [4]\n",
            "obs= [[177.2902    64.816986 177.35562   64.830376]] reward= [-1.] done= [False] info= [{'current_distance': 3.4464721895103203}]\n",
            "Step 101\n",
            "Action:  [4]\n",
            "obs= [[173.78133   64.816574 177.35562   64.830376]] reward= [-1.] done= [False] info= [{'current_distance': 169.71620513085142}]\n",
            "Step 201\n",
            "Action:  [4]\n",
            "obs= [[170.27249   64.816154 177.35562   64.830376]] reward= [-1.] done= [False] info= [{'current_distance': 336.1853771020537}]\n",
            "Step 301\n",
            "Action:  [4]\n",
            "obs= [[166.7637    64.81574  177.35562   64.830376]] reward= [-1.] done= [False] info= [{'current_distance': 502.39829497780033}]\n",
            "Step 401\n",
            "Action:  [4]\n",
            "obs= [[163.25497   64.81533  177.35562   64.830376]] reward= [-1.] done= [False] info= [{'current_distance': 668.2251668242375}]\n",
            "Step 501\n",
            "Action:  [4]\n",
            "obs= [[159.7463    64.81492  177.35562   64.830376]] reward= [-1.] done= [False] info= [{'current_distance': 833.5372667266917}]\n",
            "Step 601\n",
            "Action:  [4]\n",
            "obs= [[156.23769   64.8145   177.35562   64.830376]] reward= [-1.] done= [False] info= [{'current_distance': 998.2058013535124}]\n",
            "Step 701\n",
            "Action:  [4]\n",
            "obs= [[152.72911   64.81409  177.35562   64.830376]] reward= [-1.] done= [False] info= [{'current_distance': 1162.1016893444541}]\n",
            "Step 801\n",
            "Action:  [4]\n",
            "obs= [[149.22061   64.813675 177.35562   64.830376]] reward= [-1.] done= [False] info= [{'current_distance': 1325.0954607900524}]\n",
            "Step 901\n",
            "Action:  [4]\n",
            "obs= [[145.71214   64.813255 177.35562   64.830376]] reward= [-1.] done= [False] info= [{'current_distance': 1487.057188114936}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mean_reward, std_reward = evaluate_policy(dqn_model, dqn_model.get_env(), deterministic=True, n_eval_episodes=1)\n",
        "\n",
        "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 371
        },
        "id": "AfJFF3qrlz-l",
        "outputId": "d15f67f9-cf92-44a6-d98c-1630a17c469a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-ddb6ce04d17d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmean_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd_reward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdqn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdqn_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_eval_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines/common/evaluation.py\u001b[0m in \u001b[0;36mevaluate_policy\u001b[0;34m(model, env, n_eval_episodes, deterministic, render, callback, reward_threshold, return_episode_rewards)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mepisode_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mepisode_reward\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/dqn.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, observation, state, mask, deterministic)\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobservation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvectorized_env\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/stable_baselines/deepq/policies.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, obs, state, mask, deterministic)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions_proba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_proba\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-1.15.2/python3.7/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}